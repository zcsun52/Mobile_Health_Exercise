{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from Lilygo.Recording import Recording\n",
    "from Lilygo.Dataset import Dataset\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import entropy\n",
    "from scipy import signal\n",
    "from scipy.signal import welch\n",
    "from scipy.fftpack import fft\n",
    "from scipy.stats import entropy, kurtosis, skew\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from Lilygo.Recording import Recording, data_integrity\n",
    "from Lilygo.Dataset import Dataset\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for stepcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function aims to find the component caused by gravity from data, which means the signal around 0 Hz\n",
    "def get_gravity(data):\n",
    "    filtered_data = np.zeros_like(data)\n",
    "    # Parameters in IIR filter\n",
    "    alpla = [1, -1.979133761292768, 0.979521463540373]\n",
    "    beta = [0.000086384997973502, 0.00012769995947004, 0.000086384997973502]\n",
    "    # Formula of IIR filter\n",
    "    for i in range(2, len(data)):\n",
    "        filtered_data[i] = alpla[0] * (data[i] * beta[0] + data[i-1] * beta[1] + data[i-2] * beta[2] - filtered_data[i-1] * alpla[1] - filtered_data[i-2] * alpla[2])\n",
    "    return filtered_data\n",
    "\n",
    "# This function aims to realize a low-pass filter with cutoff frequency = 1 Hz. Because according to massive amounts of data, the general \n",
    "# minimum frequency of human walking is about 1 Hz\n",
    "def get_highpass(data):\n",
    "    filtered_data = np.zeros_like(data)  # filtered_data\n",
    "    alpla = [1, -1.905384612118461, 0.910092542787947]\n",
    "    beta = [0.953986986993339, -1.907503180919730, 0.953986986993339]\n",
    "    for i in range(2, len(data)):\n",
    "        filtered_data[i] = alpla[0] * (data[i] * beta[0] + data[i-1] * beta[1] + data[i-2] * beta[2] - filtered_data[i-1] * alpla[1] - filtered_data[i-2] * alpla[2])\n",
    "    return filtered_data\n",
    "\n",
    "# This funciton aims to realize a high-pass filter with cutoff frequency = 5 Hz. Because according to massive amounts of data, the general \n",
    "# maximum frequency of human walking is about 5 Hz\n",
    "def get_lowpass(data):\n",
    "    filtered_data = np.zeros_like(data)  # filtered_data\n",
    "    alpla = [1, -1.80898117793047, 0.827224480562408]\n",
    "    beta = [0.096665967120306, -0.172688631608676, 0.095465967120306]\n",
    "    for i in range(2, len(data)):\n",
    "        filtered_data[i] = alpla[0] * (data[i] * beta[0] + data[i-1] * beta[1] + data[i-2] * beta[2] - filtered_data[i-1] * alpla[1] - filtered_data[i-2] * alpla[2])\n",
    "    return filtered_data\n",
    "\n",
    "def pre_process(data):\n",
    "    # Find the component caused by gravity from data and remove it from the singanl\n",
    "    data_gravity = get_gravity(data)\n",
    "    data_user = data - data_gravity\n",
    "    # Get user's acceleration along the gravity direction by dot product\n",
    "    data_acc = data_user * data_gravity\n",
    "    # Add low pass and high pass filter to reduce noise in signal (possible human walking rate:1 - 5Hz)\n",
    "    data_filtered = get_highpass(data_acc)\n",
    "    data_filtered = get_lowpass(data_filtered)\n",
    "    return data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_nonoverlap(data,sampling_rate, std_win):\n",
    "    # Calculate window size\n",
    "    window_size = round(std_win*sampling_rate)\n",
    "    segment_trace = [data[s:s+window_size] for s in range(0, len(data)-window_size, window_size)]\n",
    "    return segment_trace\n",
    "\n",
    "def check_moving(magn_data):\n",
    "    acc_mag_mean = np.mean(abs(magn_data))\n",
    "    moving = False\n",
    "    # Append the features to the list\n",
    "    if acc_mag_mean > 0.027:\n",
    "        moving = True\n",
    "    return moving\n",
    "\n",
    "def normalize(data):\n",
    "    min_val = min(data)\n",
    "    max_val = max(data)\n",
    "    if min_val == max_val:\n",
    "        normalized_data = [0 for x in data]\n",
    "    else:\n",
    "        normalized_data = [(x - min_val) / (max_val - min_val)  - 0.5 for x in data]\n",
    "    return normalized_data\n",
    "\n",
    "# This function aims to find peak locations and corresponding values in the signal with the function signal.find_peaks\n",
    "def get_peaks(input_signal, prominence):\n",
    "    peak_locations, _ = signal.find_peaks(input_signal, prominence=prominence)\n",
    "    peak_values = input_signal[peak_locations]\n",
    "    return peak_locations, peak_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model for magnitute data\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Add dropout layer with 50% dropout probability\n",
    "        self.fc1 = nn.Linear(64 * 247, 255)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(255, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location------------------------------------------\n",
    "location_xgboost_model = joblib.load('group00_model_1.joblib')\n",
    "# activity------------------------------------------\n",
    "activity_xgb_model = joblib.load('group00_model_2.joblib')\n",
    "# path---------------------------------------------\n",
    "path_cnn_model = torch.load('group00_model_3.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_overlap(data,sampling_rate, std_win):\n",
    "    # Calculate raw magnitude of accelerometer signal\n",
    "    # amagn_acc = [np.sqrt(a**2+trace.data['ay'].values[i]**2+trace.data['az'].values[i]**2)for i, a in enumerate(trace.data['ay'].values)]\n",
    "    # Pre-process data\n",
    "    data_seg = pre_process(data)\n",
    "    # Calculate window size\n",
    "    window_size = round(std_win*sampling_rate)\n",
    "    segment_trace = [data_seg[s:s+window_size] for s in range(0, len(data_seg)-window_size, round(window_size/2))]\n",
    "    return segment_trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers in bearing\n",
    "def remove_outliers(data):\n",
    "    # Convert list to numpy array for easier mathematical operations\n",
    "    data = np.array(data)\n",
    "    # Calculate the Median of the data\n",
    "    median = np.median(data)\n",
    "    # Calculate the absolute deviation from the median\n",
    "    absolute_deviation = np.abs(data - median)\n",
    "    # Calculate the Median Absolute Deviation (MAD)\n",
    "    mad = np.median(absolute_deviation)\n",
    "    # Identify the outliers using a threshold (typically 2.5 or 3)\n",
    "    outliers = absolute_deviation / mad > 2.5\n",
    "    # Replace outliers with NaN\n",
    "    data[outliers] = np.nan\n",
    "    # Interpolate to replace NaNs with reasonable values\n",
    "    nans, x = np.isnan(data), lambda z: z.nonzero()[0]\n",
    "    data[nans] = np.interp(x(nans), x(~nans), data[~nans])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_watch_loc(featured_trace):\n",
    "    # Create the XGBoost DMatrix object for the test data\n",
    "    dtest = xgb.DMatrix(featured_trace)\n",
    "\n",
    "    # Make predictions on the test set and evaluate the model\n",
    "    y_pred = location_xgboost_model.predict(dtest)\n",
    "    count_1 = np.count_nonzero(y_pred == 1)\n",
    "    count_2 = np.count_nonzero(y_pred == 2)\n",
    "    count_0 = len(y_pred) - count_1 - count_2\n",
    "    count = np.array([count_0, count_1, count_2])\n",
    "    y_final = np.argmax(count)\n",
    "    return y_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_idx(magn_mag):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    X_trace = torch.tensor(np.expand_dims(magn_mag, axis=(0, -1)), dtype=torch.float32)\n",
    "    X_trace = X_trace.permute(0, 2, 1).to(device)\n",
    "    \n",
    "    path_cnn_model.to(device)\n",
    "    path_cnn_model.eval()\n",
    "    path_idx = int(np.argmax(path_cnn_model(X_trace).cpu().detach().numpy()))\n",
    "    return path_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step_count(trace):\n",
    "    # Calculate raw magnitude of accelerometer signal\n",
    "    amagn = [np.sqrt(a**2+trace.data['ay'].values[i]**2+trace.data['az'].values[i]**2)for i, a in enumerate(trace.data['ax'].values)]\n",
    "    # Filter the signal to get more accurate results -----------------------------------------------------------\n",
    "  \n",
    "    data_filtered = pre_process(amagn)\n",
    "    # Use convolution to reduce noise in signal again\n",
    "    filter_window_size = 40\n",
    "    data_filtered = np.convolve(data_filtered, np.ones((filter_window_size,))/filter_window_size, mode='valid')\n",
    "    # Find peaks in the filtered signal and realize our stepcount -----------------------------------------------\n",
    "    # Segment data into windows --------------------------------------------------------------------------------\n",
    "    std_win = 1 # length of window in seconds\n",
    "    sampling_rate = 200\n",
    "    data_segmented = get_segment_nonoverlap(data_filtered, sampling_rate, std_win)\n",
    "    # Normalize data in each windows ---------------------------------------------------------------------------\n",
    "    win_size = round(std_win * sampling_rate)\n",
    "    normalized_data = data_filtered.copy()\n",
    "    for i, seg in enumerate(data_segmented):\n",
    "        if check_moving(seg):\n",
    "            normalized_data[i*win_size:i*win_size + win_size] = normalize(seg)\n",
    "        else:\n",
    "            normalized_data[i*win_size:i*win_size + win_size] = 0\n",
    "    # check data after last window\n",
    "    if np.shape(data_segmented)[0]*win_size < len(data_filtered):\n",
    "        seg = data_filtered[np.shape(data_segmented)[0]*win_size:]\n",
    "        if check_moving(seg):\n",
    "                normalized_data[i*win_size:i*win_size + len(seg)] = normalize(seg)\n",
    "        else:\n",
    "            normalized_data[i*win_size:i*win_size + len(seg)] = 0\n",
    "\n",
    "    # Find peaks in the filtered signal and realize our stepcount -----------------------------------------------\n",
    "    prominence = 0.4\n",
    "    peak_locations, _ = get_peaks(normalized_data, prominence)\n",
    "    stepCount = len(peak_locations)\n",
    "    \n",
    "    return stepCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity(featured_trace):\n",
    "    stand, walk, run, cycle = 0, 0, 0, 0\n",
    "    # Create the XGBoost DMatrix object for the test data\n",
    "    dtest = xgb.DMatrix(featured_trace)\n",
    "    # Make predictions on the test set and evaluate the model\n",
    "    y_pred = activity_xgb_model.predict(dtest)\n",
    "    # filter prediction by 60s\n",
    "    # Sliding window: 60s\n",
    "    std_win = 10\n",
    "    n = round (60 / std_win * 2 - 1)\n",
    "    y_pred_count = np.zeros(4)\n",
    "    y_pred_60 = y_pred.copy()\n",
    "    for s in range(0, len(y_pred) - n, int(n/2+1)):\n",
    "        # window = 60s \n",
    "        windowed_label = y_pred[s : s+n]\n",
    "        for j in range(n): \n",
    "            # Find the label that appears the most\n",
    "            for k in range(4):\n",
    "                if windowed_label[j] == k:\n",
    "                    y_pred_count[k]+=1\n",
    "        label_argmax = np.where(y_pred_count == np.max(y_pred_count))\n",
    "        # print(label_argmax)\n",
    "        if len(label_argmax)==1:\n",
    "            y_pred_60[s : s+n] = np.argmax(y_pred_count)\n",
    "    \n",
    "    # remove duplicated elements\n",
    "    predicted = list(set(y_pred_60))\n",
    "    if 0 in predicted:\n",
    "        stand = 1\n",
    "    if 1 in predicted:\n",
    "        walk = 1\n",
    "    if 2 in predicted:\n",
    "        run = 1\n",
    "    if 3 in predicted:\n",
    "        cycle = 1    \n",
    "    return stand, walk, run, cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to test data dir\n",
    "dir_traces = '/kaggle/input/mobile-health-2023-path-detection/data/test'\n",
    "filenames = [join(dir_traces, f) for f in listdir(dir_traces) if isfile(join(dir_traces, f)) and f[-5:] == '.json']\n",
    "filenames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process traces:  10 / 376\n",
      "Process traces:  20 / 376\n",
      "Process traces:  30 / 376\n",
      "Process traces:  40 / 376\n",
      "Process traces:  50 / 376\n",
      "Process traces:  60 / 376\n",
      "Process traces:  70 / 376\n",
      "Process traces:  80 / 376\n",
      "Process traces:  90 / 376\n",
      "Process traces:  100 / 376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\szzcc\\.conda\\envs\\mhealth23\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process traces:  110 / 376\n",
      "Process traces:  120 / 376\n",
      "Process traces:  130 / 376\n",
      "Process traces:  140 / 376\n",
      "Process traces:  150 / 376\n",
      "Process traces:  160 / 376\n",
      "Process traces:  170 / 376\n",
      "Process traces:  180 / 376\n",
      "Process traces:  190 / 376\n",
      "Process traces:  200 / 376\n",
      "Process traces:  210 / 376\n",
      "Process traces:  220 / 376\n",
      "Process traces:  230 / 376\n",
      "Process traces:  240 / 376\n",
      "Process traces:  250 / 376\n",
      "Process traces:  260 / 376\n",
      "Process traces:  270 / 376\n",
      "Process traces:  280 / 376\n",
      "Process traces:  290 / 376\n",
      "Process traces:  300 / 376\n",
      "Process traces:  310 / 376\n",
      "Process traces:  320 / 376\n",
      "Process traces:  330 / 376\n",
      "Process traces:  340 / 376\n",
      "Process traces:  350 / 376\n",
      "Process traces:  360 / 376\n",
      "Process traces:  370 / 376\n"
     ]
    }
   ],
   "source": [
    "# load saved feature\n",
    "feature_all_trace = np.load(\"group00_features.npy\", allow_pickle=True).item()\n",
    "# Loop through all traces and calculate the step count for each trace\n",
    "solution_file = []\n",
    "for idx, filename in enumerate(filenames):\n",
    "    trace_id = ''.join([*filename][-8:-5])\n",
    "    \n",
    "    trace = Recording(filename, no_labels=True, mute=True)\n",
    "    categorization_results = {'watch_loc': 0, 'path_idx': 0, 'step_count': 0, 'stand': 0, 'walk': 0, 'run': 0, 'cycle': 0}\n",
    "\n",
    "    #\n",
    "    # Your algorithm goes here\n",
    "    # Make sure, you do not use the gps data and are tolerant for missing data (see task set).\n",
    "    # Your program must not crash when single smartphone data traces are missing.\n",
    "    #\n",
    "    feature_cur_trace = feature_all_trace[trace_id]\n",
    "    categorization_results['watch_loc'] = get_watch_loc(feature_cur_trace[\"location\"])\n",
    "    categorization_results['path_idx'] = get_path_idx(feature_cur_trace[\"path_idx\"])\n",
    "    categorization_results['step_count'] = get_step_count(trace)\n",
    "    categorization_results['stand'], categorization_results['walk'], categorization_results['run'], categorization_results['cycle'] = get_activity(feature_cur_trace[\"activity\"])\n",
    "\n",
    "\n",
    "    # Append your calculated results and the id of each trace and category to the solution file\n",
    "    for counter_label, category in enumerate(categorization_results):\n",
    "        solution_file.append([trace_id + f'_{counter_label+1}', categorization_results[category]])\n",
    "    # show progress\n",
    "    if (idx+1)%10 == 0:\n",
    "        print(\"Process traces: \", idx+1, '/', len(filenames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write the detected step counts into a .csv file to then upload the .csv file to Kaggle\n",
    "# When cross-checking the .csv file on your computer, we recommend using the text editor and NOT excel so that the results are displayed correctly\n",
    "# IMPORTANT: Do NOT change the name of the columns ('Id' and 'Category') of the .csv file\n",
    "submission_file_df = pd.DataFrame(np.asarray(solution_file), columns=['Id', 'Category'])\n",
    "submission_file_df.to_csv('submission.csv', header=['Id', 'Category'], index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
